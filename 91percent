# Sequence Creation
# =======================
SEQ_LENGTH = 90  # Increased to capture longer-term patterns
X, y = create_sequences(scaled_data, SEQ_LENGTH)

# Reshape for LSTM input (samples, time steps, features)
X = X.reshape(X.shape[0], X.shape[1], 1)

# =======================
# Train-Test Split
# =======================
train_size = int(len(X) * 0.7)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]
# =======================
# LSTM Model Design
# =======================
model = Sequential([
    Input(shape=(X_train.shape[1], X_train.shape[2])),  # Correct Input Layer
    LSTM(64, return_sequences=True),
    Dropout(0.2),
    LSTM(64, return_sequences=True),   # Added LSTM for better feature extraction
    Dropout(0.2),
    LSTM(32, return_sequences=False),  # Final LSTM layer for sequence summarization
    Dense(16, activation='relu'),
    Dense(1)  # Output layer
])
model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse')

# =======================
# Callbacks for Improvement
# =======================
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

model_checkpoint = ModelCheckpoint(
    'best_lstm_model.h5', 
    monitor='val_loss', 
    save_best_only=True, 
    mode='min', 
    verbose=1
)

# Model Training
# =======================
history = model.fit(
    X_train, y_train,
    epochs=20,
    batch_size=32,      # Keeping batch size small to capture finer details
    validation_data=(X_test, y_test),
    callbacks=[early_stopping, model_checkpoint],
    verbose=1
)
