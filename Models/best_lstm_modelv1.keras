def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:i + seq_length])
        y.append(data[i + seq_length][0])  # Predicting 'DATA (psi)'
    return np.array(X), np.array(y)

SEQ_LENGTH = 90 #Increased to capture longer-term patterns
X, y = create_sequences(data_scaled, SEQ_LENGTH)

# Reshape for LSTM input (samples, time steps, features)
X = X.reshape(X.shape[0], X.shape[1], 1)

# =======================
# Train-Test Split
# =======================
train_size = int(len(X) * 0.9)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)


from tensorflow.keras.losses import Huber
from tensorflow import keras
# LSTM Model Design
model = Sequential()
model.add(LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dropout(0.4))
model.add(LSTM(64, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(32, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(1))

# Compile with a reduced learning rate for better convergence since our data is viotile 
model.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss=Huber(delta=1.5), metrics=['mae'])

# Early stopping function to avoid overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# to save the best lstm model
model_checkpoint = keras.callbacks.ModelCheckpoint(
    filepath="best_lstm_modelv1.keras",  # Recommended ".keras" format instead of ".h5"
    monitor="val_loss",
    save_best_only=True,
    mode="min",
    verbose=1, 
)

reduce_lr = ReduceLROnPlateau(
    monitor="val_loss",    # Monitor the validation loss
    factor=0.005,            # Reduce the learning rate by a factor of 0.005
    patience=3,            # Wait for 3 epochs before reducing the learning rate
    verbose=1,             # Print the change of the learning rate
    min_lr=1e-6            # Set a lower bound for the learning rate
)

# Model Training
history = model.fit(
    X_train, y_train,
    epochs=20,
    batch_size=32,      # Keeping batch size small to capture finer details
    validation_data=(X_test, y_test),
    callbacks=[early_stopping, model_checkpoint],
    verbose=1
)
