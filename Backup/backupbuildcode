import tensorflow as tf
def build_model(hp):
    model = Sequential()

    # Hyperparameter choices
    hp_activation = hp.Choice('activation', values=['elu', 'relu'])
    dense_units = hp.Int('dense_units', min_value=16, max_value=64, step=16)

    # First LSTM layer
    model.add(LSTM(
        units=hp.Int('lstm_units_1', min_value=64, max_value=256, step=64),
        return_sequences=True,  # Corrected 'return_sequence' → 'return_sequences'
        input_shape=(SEQ_LENGTH, 1)
    ))
    model.add(Dropout(hp.Float('dropout_1', min_value=0.2, max_value=0.4, step=0.1)))  # Fixed hp.float()

    # Second LSTM layer
    model.add(LSTM(
        units=hp.Int('lstm_units_2', min_value=64, max_value=128, step=32),
        return_sequences=True
    ))
    model.add(Dropout(hp.Float('dropout_2', min_value=0.2, max_value=0.4, step=0.1)))

    # Third LSTM layer
    model.add(LSTM(
        units=hp.Int('lstm_units_3', min_value=16, max_value=64, step=16),
        return_sequences=False  # Ensures correct output shape before Dense layers
    ))

    # Dense layer with activation function (elu/relu)
    model.add(Dense(dense_units, activation=hp_activation))

    # Output layer (Regression task)
    model.add(Dense(1))

    # Hyperparameter tuning for learning rate
    learning_rate = hp.Float('learning_rate', 1e-5, 1e-3, sampling='log')
    optimizer = Adam(learning_rate=learning_rate)

    # Compile model
    model.compile(
        optimizer=optimizer,
        loss=Huber(delta=hp.Float('huber_delta', 0.5, 1.5, step=0.5)),  # Corrected hp.float() → hp.Float()
        metrics=['mae']
    )

    return model


